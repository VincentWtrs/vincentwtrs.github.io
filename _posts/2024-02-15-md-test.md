---
layout: post
title: Markdown test
subtitle: test
tags: [machine learning]
comments: false
author: Vincent Wauters
published: true
---


# Intro
Suppose you have built a fantastic machine learning model for predicting the selling price of a given house. Between the following two statements, what will make the biggest impression?

_'I predict that this house has a value of 450.000 euro'_

_or_

_'I predict that this house has a value between 435.000 euro and 465.000 euro with 90% certainty_'

While the boldness of the first statement might impress some people - certainly in the area of real estate - the later statement does convey information about the magnitude of uncertainty, which is incredibly useful. This is uncertainty quantification (UQ), for regression cases this comes down to constructing _prediction intervals_ (PI) and for classification to _prediction sets_.

Up until recently, the most common ways of uncertainty quantification were based on unrealistic assumptions (such as normality) amongst others and led to unsatisfactory results. Luckily, there is a 'new(er)' kid on the block called _conformal prediction_ or _conformal inference_, which is a very powerful and model-agnostic method to construct reliable prediction intervals or predictions sets without any distributional assumptions. 

A specific type of conformal prediction leverages the power of (conditional) quantile regression and provides a rather elegant way of constructing prediction intervals for regression cases that have nice properties, this will be the focus of this post.

# Conformal Quantile Regression: Concepts

## Background

Before diving straight into the specific type of conformal prediction of focus, let's quickly provide some more context about conformal prediction. Research in this field has been going on for some decades, mainly focused around the work of Vladimir Vovk and his colleagues.

Types of conformal prediction are often divided into 3 categories:
1. Full conformal prediction (the original implementation)
2. Split-conformal prediction
3. Cross-conformal prediction

Full conformal prediction, which is the original way of conducting conformal prediction following Vovk's research, requires vof refitting a very large amount of models, hence is computationally heavy. The latter two make use of data splitting that drive down the computation load substantially. 

## Conditional Quantile Regression and the Pinball Loss

For the case of regression with continuous outcomes, __(conditional) quantile regression__ is well known, but not particularly popular. While most regression models estimate the conditional mean, quantile regression aims to estimate a certain conditional quantile $q_{\tau}$ with $\tau \in [0, 1]$. The most common example would be to estimate the conditional $\tau=0.5$ quantile $q_{0.5}$, which is the conditional median. And yes indeed, one could estimate two conditional quantile models, one for lower bound and one for an upper bound to try to achieve valid prediction intervals. More on this a little later!

How can one estimate a conditional quantile instead of the (usual) conditional mean, you ask? Quantile regression achieves this by using a specific family of loss functions. Just like estimating the conditional mean is done by minimizing the mean squared error MSE loss, estimating conditional quantiles is achieved by minimizing the family of __pinball losses__ or _hockey stick losses_. Take the loss of the target and the estimated quantile using the features $\mathbf{x}$ as $q(\mathbf{x})$, for a conditional quantile $q_{\tau}$ with $\tau \in [0, 1]$

$$
\rho_{\tau}(y, \hat{q}(\mathbf{x})) :=
\begin{cases}
    \tau(y-\hat{q}(\mathbf{x})), & \text{if } y-\hat{q}(\mathbf{x}) > 0\\
    (1-\tau)(y - \hat{q}(\mathbf{x})), & \text{otherwise.}
\end{cases}
$$
